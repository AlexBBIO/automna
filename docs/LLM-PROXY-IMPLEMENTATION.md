# LLM Proxy Implementation

## Status: âœ… Complete (2026-02-03)

The LLM proxy is fully implemented and working.

## What's Implemented

### Proxy Endpoint
- **Path:** `/api/llm/v1/messages`
- **Runtime:** Vercel Edge
- **Auth:** Gateway token via `x-api-key` header
- **Features:** Streaming support, usage logging, error handling

### Machine Configuration
- Custom `automna` provider in OpenClaw config
- Gateway token as API key (not real Anthropic key)
- Auto-generated by `docker/entrypoint.sh`

### Usage Tracking
- All requests logged to Turso `llm_usage` table
- Tracks: user, model, tokens, cost, duration, errors

### Security
- Real Anthropic key only in Vercel env vars
- Users authenticate with per-machine gateway tokens
- Cloudflare WAF exception for `/api/llm/*` path

## Issues Resolved (2026-02-03)

1. **Cloudflare WAF blocking requests**
   - Large LLM payloads with code triggered security rules
   - Fixed by adding WAF skip rule for `/api/llm/*`

2. **Fly secrets overriding machine config**
   - Old `ANTHROPIC_API_KEY` secret had real key
   - Fixed by removing secret via GraphQL API

3. **Vercel env not deployed**
   - New API key set but not redeployed
   - Fixed by running `vercel --prod`

## Files

| File | Purpose |
|------|---------|
| `landing/src/app/api/llm/v1/messages/route.ts` | Proxy endpoint |
| `landing/src/app/api/llm/_lib/auth.ts` | Token validation |
| `landing/src/app/api/llm/_lib/usage.ts` | Usage logging |
| `landing/src/app/api/llm/_lib/rate-limit.ts` | Rate limiting |
| `docker/entrypoint.sh` | Config generation |

## Next Steps

- [ ] Implement rate limiting based on user plan
- [ ] Add usage dashboard in admin panel
- [ ] Alert on unusual usage patterns
