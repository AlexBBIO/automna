> **⚠️ OUTDATED:** This document describes the legacy Vercel proxy. The current proxy runs on Fly.io. See [API-PROXIES.md](API-PROXIES.md) for the current architecture.

# LLM Proxy

All LLM calls from user machines route through our proxy at `automna.ai/api/llm`. 

**Why:**
- Usage tracking per user (tokens, cost, latency)
- Rate limiting (planned)
- Security - users never see our real Anthropic API key

## Architecture

```
User Machine                     Automna Proxy                 Anthropic
(OpenClaw)                      (Vercel Edge)
    │                                │                              │
    │ POST /v1/messages              │                              │
    │ x-api-key: <gateway-token>     │                              │
    │──────────────────────────────► │                              │
    │                                │                              │
    │                     1. Validate gateway token                 │
    │                     2. Lookup user in Turso                   │
    │                     3. Check rate limits                      │
    │                                │                              │
    │                                │ POST /v1/messages             │
    │                                │ x-api-key: <real-key>         │
    │                                │─────────────────────────────►│
    │                                │                              │
    │                                │◄─────────────────────────────│
    │                     4. Log usage to Turso                     │
    │◄──────────────────────────────│                              │
```

## Configuration

### User Machines (Fly.io)

Each machine has a custom `automna` provider in `/home/node/.openclaw/clawdbot.json`:

```json
{
  "models": {
    "providers": {
      "automna": {
        "baseUrl": "https://automna.ai/api/llm",
        "apiKey": "<gateway-token>",
        "api": "anthropic-messages",
        "models": [
          {"id": "claude-opus-4-5", "name": "Claude Opus 4.5"}
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "automna/claude-opus-4-5"
      }
    }
  }
}
```

**Key points:**
- `apiKey` = gateway token (NOT a real Anthropic key)
- Config is generated by `docker/entrypoint.sh` on machine startup
- Real Anthropic key exists **only** in Vercel env vars

### Machine Environment Variables

| Variable | Value | Purpose |
|----------|-------|---------|
| `OPENCLAW_GATEWAY_TOKEN` | UUID | Primary auth token |
| `ANTHROPIC_API_KEY` | Same as gateway token | SDK fallback (should match gateway token) |

**⚠️ IMPORTANT:** `ANTHROPIC_API_KEY` on machines must be the gateway token, not a real Anthropic key. If set to a real key, requests bypass the proxy.

### Vercel Environment Variables

| Variable | Purpose |
|----------|---------|
| `ANTHROPIC_API_KEY` | The real Anthropic API key (only place it exists) |

## Files

| File | Purpose |
|------|---------|
| `landing/src/app/api/llm/v1/messages/route.ts` | Main proxy endpoint |
| `landing/src/app/api/llm/_lib/auth.ts` | Gateway token validation |
| `landing/src/app/api/llm/_lib/usage.ts` | Turso usage logging |
| `landing/src/app/api/llm/_lib/rate-limit.ts` | Rate limiting |
| `docker/entrypoint.sh` | Generates config on machine startup |

## Usage Tracking

Logged to `llm_usage` table in Turso:

| Column | Description |
|--------|-------------|
| `user_id` | Clerk user ID |
| `model` | Model name (e.g., `claude-opus-4-5`) |
| `input_tokens` | Input token count |
| `output_tokens` | Output token count |
| `cost_microdollars` | Calculated cost |
| `duration_ms` | Request duration |
| `error` | Error message if failed |

## Cloudflare WAF

The proxy endpoint must bypass Cloudflare's WAF because LLM requests contain code-like content that triggers security rules.

**Required Cloudflare rule:**
- Path: `/api/llm/*`
- Action: Skip all managed rules

Without this, requests return `403 Your request was blocked`.

## Troubleshooting

### Requests hitting wrong API key

**Symptom:** Usage appears on old/direct Anthropic key instead of proxy key

**Causes:**
1. **Fly secrets override env vars** - Check `fly secrets list -a <app>`. Remove any `ANTHROPIC_API_KEY` secret.
2. **Machine not restarted** - After config changes, restart: `fly machines restart <id> -a <app>`
3. **Vercel not redeployed** - Env var changes require new deployment

**Fix:**
```bash
# Remove secret if exists
fly secrets unset ANTHROPIC_API_KEY -a automna-u-<id>

# Update machine config directly via API
curl -X POST "https://api.machines.dev/v1/apps/<app>/machines/<machine-id>" \
  -H "Authorization: Bearer $FLY_API_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"config": {"env": {"ANTHROPIC_API_KEY": "<gateway-token>", ...}}}'

# Redeploy Vercel if env changed
vercel --prod
```

### 403 Your request was blocked

**Cause:** Cloudflare WAF blocking the request

**Fix:** Add WAF exception for `/api/llm/*` path (see Cloudflare section above)

### 0 tokens logged / Empty responses

**Cause:** Previously caused by SSE chunk boundaries splitting events. Fixed 2026-02-03.

**If still happening, check:**
1. Verify Vercel `ANTHROPIC_API_KEY` is valid
2. Check Turso `llm_usage.error` column for error messages
3. Check Vercel function logs

## Security Model

| Secret | Location | Who can see |
|--------|----------|-------------|
| Real Anthropic Key | Vercel env only | Operators only |
| Gateway Token | Turso + machine config | Per-user, validated |

Users cannot:
- See the real Anthropic API key
- Bypass rate limits (no direct Anthropic access)
- Impersonate other users (token tied to their machine)
